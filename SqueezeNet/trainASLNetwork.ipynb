{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loading images\n",
      "loading evaluation images\n",
      "loading train/test images\n",
      "done loading images\n",
      "Total number of symbols:  29\n",
      "Number of training images:  78300\n",
      "Number of testing images:  8700\n",
      "Number of evaluation images:  812\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "29\n",
      "Epoch 1/5\n",
      "78300/78300 [==============================] - 203s 3ms/sample - loss: 1.2512 - acc: 0.6048\n",
      "Epoch 2/5\n",
      "78300/78300 [==============================] - 194s 2ms/sample - loss: 0.2459 - acc: 0.9197\n",
      "Epoch 3/5\n",
      "78300/78300 [==============================] - 192s 2ms/sample - loss: 0.1269 - acc: 0.9591\n",
      "Epoch 4/5\n",
      "78300/78300 [==============================] - 195s 2ms/sample - loss: 0.0863 - acc: 0.9727\n",
      "Epoch 5/5\n",
      "78300/78300 [==============================] - 197s 3ms/sample - loss: 0.0637 - acc: 0.9789\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 64, 64, 32)   896         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 32)   9248        leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 32)   9248        leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 32, 32, 32)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 32)   9248        leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 32, 32, 32)   0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 32)   9248        leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 32, 32, 32)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 16, 16, 32)   0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 16, 16, 32)   320         leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 32)   1056        leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 16, 16, 32)   320         leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 32)   128         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 32)   1056        leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           leaky_re_lu_39[0][0]             \n",
      "                                                                 leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 32)           0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          4224        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 29)           3741        dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,365\n",
      "Trainable params: 58,109\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 804), started 0:33:21 ago. (Use '!kill 804' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f96709af940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_test.jpg (64, 64, 3)\n",
      "0\n",
      "B_test.jpg (64, 64, 3)\n",
      "1\n",
      "C_test.jpg (64, 64, 3)\n",
      "2\n",
      "D_test.jpg (64, 64, 3)\n",
      "3\n",
      "E_test.jpg (64, 64, 3)\n",
      "4\n",
      "F_test.jpg (64, 64, 3)\n",
      "5\n",
      "G_test.jpg (64, 64, 3)\n",
      "6\n",
      "H_test.jpg (64, 64, 3)\n",
      "7\n",
      "I_test.jpg (64, 64, 3)\n",
      "8\n",
      "J_test.jpg (64, 64, 3)\n",
      "9\n",
      "K_test.jpg (64, 64, 3)\n",
      "10\n",
      "L_test.jpg (64, 64, 3)\n",
      "11\n",
      "M_test.jpg (64, 64, 3)\n",
      "12\n",
      "N_test.jpg (64, 64, 3)\n",
      "13\n",
      "O_test.jpg (64, 64, 3)\n",
      "14\n",
      "P_test.jpg (64, 64, 3)\n",
      "15\n",
      "Q_test.jpg (64, 64, 3)\n",
      "16\n",
      "R_test.jpg (64, 64, 3)\n",
      "17\n",
      "S_test.jpg (64, 64, 3)\n",
      "18\n",
      "T_test.jpg (64, 64, 3)\n",
      "19\n",
      "U_test.jpg (64, 64, 3)\n",
      "20\n",
      "V_test.jpg (64, 64, 3)\n",
      "21\n",
      "W_test.jpg (64, 64, 3)\n",
      "22\n",
      "X_test.jpg (64, 64, 3)\n",
      "23\n",
      "Y_test.jpg (64, 64, 3)\n",
      "24\n",
      "Z_test.jpg (64, 64, 3)\n",
      "25\n",
      "nothing_test.jpg (64, 64, 3)\n",
      "27\n",
      "space_test.jpg (64, 64, 3)\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import datetime\n",
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Add, Input, Dense, Dropout, Flatten, GlobalAveragePooling2D, DepthwiseConv2D, BatchNormalization, LeakyReLU, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_dir = \"./asl_alphabet_train/asl_alphabet_train\"\n",
    "eval_dir = \"./asl_alphabet_test/asl_alphabet_test\"\n",
    "batch_size = 64\n",
    "imageSize = 64\n",
    "target_dims = (64, 64, 3)\n",
    "num_classes = 29\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "#Helper function to load images from given directories\n",
    "def load_images(directory, uniq_labels, evalData=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx, label in enumerate(uniq_labels):\n",
    "        if evalData == False:\n",
    "            #i=0\n",
    "            for file in os.listdir(directory + \"/\" + label):\n",
    "                #if i>10:\n",
    "                #    break\n",
    "                filepath = directory + \"/\" + label + \"/\" + file\n",
    "                image = cv2.resize(cv2.imread(filepath), (64, 64))\n",
    "                images.append(image)\n",
    "                labels.append(idx)\n",
    "                #i=i+1\n",
    "        else:\n",
    "            for file in os.listdir(directory):\n",
    "                filepath = directory + \"/\" + file\n",
    "                image = cv2.resize(cv2.imread(filepath), (64, 64))\n",
    "                images.append(image)\n",
    "                labels.append(idx)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return (images, labels)\n",
    "\n",
    "\n",
    "def trainModel():\n",
    "    uniq_labels = sorted(os.listdir(train_dir))\n",
    "    X_eval, y_eval = None, None\n",
    "    print(\"loading evaluation images\")\n",
    "    X_eval, y_eval = load_images(eval_dir, uniq_labels, evalData=True)\n",
    "    print(\"loading train/test images\")\n",
    "    images, labels = load_images(train_dir, uniq_labels)\n",
    "\n",
    "    print(\"done loading images\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images,\n",
    "                                                        labels,\n",
    "                                                        test_size=0.1,\n",
    "                                                        stratify=labels)\n",
    "\n",
    "    n = len(uniq_labels)\n",
    "    train_n = len(X_train)\n",
    "    test_n = len(X_test)\n",
    "\n",
    "    print(\"Total number of symbols: \", n)\n",
    "    print(\"Number of training images: \", train_n)\n",
    "    print(\"Number of testing images: \", test_n)\n",
    "\n",
    "    eval_n = len(X_eval)\n",
    "    print(\"Number of evaluation images: \", eval_n)\n",
    "\n",
    "    y_train_in = y_train.argsort()\n",
    "    y_train = y_train[y_train_in]\n",
    "    X_train = X_train[y_train_in]\n",
    "    y_test_in = y_test.argsort()\n",
    "    y_test = y_test[y_test_in]\n",
    "    X_test = X_test[y_test_in]\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_eval = to_categorical(y_eval)\n",
    "    print(y_train[0])\n",
    "    print(len(y_train[0]))\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    X_eval = X_eval.astype('float32') / 255.0\n",
    "\n",
    "    inputs = Input(shape=target_dims)\n",
    "    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(inputs)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n",
    "    net = LeakyReLU()(net)\n",
    "\n",
    "    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n",
    "    net = LeakyReLU()(net)\n",
    "\n",
    "    shortcut = net\n",
    "\n",
    "    net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "    net = BatchNormalization(axis=3)(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "    net = BatchNormalization(axis=3)(net)\n",
    "    net = LeakyReLU()(net)\n",
    "\n",
    "    net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "    net = BatchNormalization(axis=3)(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n",
    "    net = BatchNormalization(axis=3)(net)\n",
    "    net = LeakyReLU()(net)\n",
    "    net = Add()([net, shortcut])\n",
    "\n",
    "    net = GlobalAveragePooling2D()(net)\n",
    "    net = Dropout(0.2)(net)\n",
    "\n",
    "    net = Dense(128, activation='relu')(net)\n",
    "    outputs = Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    hist = model.fit(X_train, y_train, epochs=5, batch_size=64, callbacks=[tensorboard_callback])\n",
    "    model.save('aslClassifier.h5')\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"now loading images\")\n",
    "    #check if saved model exists\n",
    "    foundModel = glob.glob(\"*.h5\")\n",
    "    if len(foundModel) != 0:\n",
    "        model = load_model(foundModel[0])\n",
    "        model.summary()\n",
    "        \"\"\" \n",
    "        score = model.evaluate(x=X_test, y=y_test, verbose=0)\n",
    "        print('Accuracy for test images:', round(score[1] * 100, 3), '%')\n",
    "        score = model.evaluate(x=X_eval, y=y_eval, verbose=0)\n",
    "        print('Accuracy for evaluation images:', round(score[1] * 100, 3), '%')\n",
    "\n",
    "        y_eval_pred = model.predict(X_eval, batch_size=64, verbose=0)\n",
    "        print(y_eval_pred) \"\"\"\n",
    "    else:\n",
    "        model = trainModel()\n",
    "        model.summary()\n",
    "    # now that you have loaded the network, predict a test sample\n",
    "    for file in os.listdir(eval_dir):\n",
    "        filepath = eval_dir + \"/\" + file\n",
    "        test_image = cv2.resize(cv2.imread(filepath), (64, 64))\n",
    "        test_image = test_image.astype('float32') / 255.0\n",
    "        print(file, test_image.shape)\n",
    "        print(np.argmax(model.predict(np.expand_dims(test_image, axis=0))))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
